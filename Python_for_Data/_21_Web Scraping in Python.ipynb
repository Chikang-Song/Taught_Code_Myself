{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ec9da2",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "[Tip] 다음 주제를 다룹니다.\n",
    "    <ol>\n",
    "        <li><a href=#1>BeautifulSoup</a></li>\n",
    "        <li><a href=#2>Scrapy</a></li>\n",
    "        <li><a href=#3>Selenium</a></li>\n",
    "        <li><a href=#4>Pandas.read_html()</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca3ee2",
   "metadata": {},
   "source": [
    "**웹 스크래핑(Web Scraping)**은 **웹 하베스팅(Web Harvesting)** 또는 **웹 데이터 추출(Web Data Extraction)**이라고도 불리며, 웹사이트에서 **대량의 데이터를 추출**하는 기술입니다.  \n",
    "\n",
    "웹사이트의 데이터는 **비정형(unstructured)** 데이터로 구성되어 있습니다.  \n",
    "웹 스크래핑을 사용하면 이 데이터를 **정형(structured)** 데이터 형태로 변환할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **웹 스크래핑의 주요 목적:**  \n",
    "- 웹 페이지에서 원하는 데이터를 자동으로 수집  \n",
    "- 비정형 데이터를 **데이터프레임** 또는 **데이터베이스**와 같은 정형 데이터로 변환  \n",
    "- 데이터 분석, 머신러닝, 연구 등 다양한 목적으로 활용 가능\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 **웹 스크래핑 활용 사례:**  \n",
    "- 뉴스 기사 추출  \n",
    "- 제품 가격 비교  \n",
    "- 소셜 미디어 데이터 수집  \n",
    "- 부동산 정보 수집  \n",
    "\n",
    "웹 스크래핑을 통해 인터넷에 있는 **대규모 데이터**를 효과적으로 활용할 수 있습니다! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f00e2f",
   "metadata": {},
   "source": [
    "## Importance of Web Scraping in Data Science\n",
    "\n",
    "**데이터 과학(Data Science)** 분야에서 **웹 스크래핑(Web Scraping)**은 매우 중요한 역할을 합니다.  \n",
    "다양한 목적으로 사용되며, 아래와 같은 사례들이 대표적입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **웹 스크래핑의 주요 활용 사례:**  \n",
    "\n",
    "1️⃣ **데이터 수집(Data Collection)**  \n",
    "   - 웹 스크래핑은 인터넷에서 데이터를 수집하는 **주요 방법**입니다.  \n",
    "   - 수집된 데이터는 **분석**, **연구**, **비즈니스 인사이트 도출** 등에 활용됩니다.\n",
    "\n",
    "2️⃣ **실시간 애플리케이션(Real-time Application)**  \n",
    "   - 웹 스크래핑은 **실시간 애플리케이션**에 사용됩니다.  \n",
    "   - 예시:  \n",
    "     - **날씨 업데이트**  \n",
    "     - **가격 비교 사이트**  \n",
    "     - **주식 시장 데이터 수집**  \n",
    "\n",
    "3️⃣ **머신러닝(Machine Learning)**  \n",
    "   - **머신러닝 모델**을 훈련시키기 위해 **대규모 데이터**가 필요합니다.  \n",
    "   - 웹 스크래핑을 통해 다양한 분야의 데이터를 수집하여 모델을 **훈련(training)**시킬 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **요약:**  \n",
    "웹 스크래핑은 **데이터 과학**, **실시간 시스템**, **머신러닝** 등 다양한 분야에서 **데이터 수집**을 자동화하는 핵심 기술입니다! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a922f",
   "metadata": {},
   "source": [
    "<a id=1></a>\n",
    "## Web Scraping with Python\n",
    "\n",
    "**Python은 웹 스크래핑을 위한 여러 라이브러리를 제공합니다. 다음은 그 중 몇 가지입니다:**  \n",
    "\n",
    "**BeautifulSoup:**  \n",
    "BeautifulSoup은 HTML 및 XML 파일에서 데이터를 추출하기 위해 사용되는 Python 라이브러리입니다. 페이지 소스 코드를 파싱 트리(parse tree)로 변환하여 계층적이고 읽기 쉬운 방식으로 데이터를 추출할 수 있도록 돕습니다.  \n",
    "\n",
    "다음은 BeautifulSoup을 사용한 간단한 웹 스크래핑 예제 코드입니다:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup  \n",
    "import requests  \n",
    "\n",
    "URL = \"http://www.example.com\"  \n",
    "page = requests.get(URL)  \n",
    "soup = BeautifulSoup(page.content, \"html.parser\")  \n",
    "```\n",
    "\n",
    "**설명:**  \n",
    "1️⃣ `from bs4 import BeautifulSoup` — BeautifulSoup 라이브러리를 가져옵니다.  \n",
    "2️⃣ `import requests` — 웹 페이지 요청을 위해 requests 라이브러리를 가져옵니다.  \n",
    "3️⃣ `URL = \"http://www.example.com\"` — 접속할 웹 페이지의 URL을 설정합니다.  \n",
    "4️⃣ `page = requests.get(URL)` — 해당 URL의 페이지 콘텐츠를 가져옵니다.  \n",
    "5️⃣ `soup = BeautifulSoup(page.content, \"html.parser\")` — 페이지 콘텐츠를 BeautifulSoup 객체로 파싱하여 분석할 수 있는 구조로 만듭니다.\n",
    "\n",
    "이 코드를 실행하면 `soup` 객체를 통해 웹 페이지에서 원하는 데이터를 쉽게 추출할 수 있습니다. 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35599b",
   "metadata": {},
   "source": [
    "<a id=2></a>\n",
    "### **Scrapy:**  \n",
    "Scrapy는 Python용 오픈 소스 협업 웹 크롤링 프레임워크입니다. 주로 웹사이트에서 데이터를 추출하는 데 사용되며, 크롤러를 작성하고 데이터를 수집하는 데 최적화되어 있습니다. Scrapy는 빠르고 효율적으로 여러 페이지를 자동으로 탐색할 수 있는 강력한 기능을 제공합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scrapy를 사용한 웹 크롤링 예제 코드:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = ['http://quotes.toscrape.com/tag/humor/',]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {'quote': quote.css('span.text::text').get()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **설명:**  \n",
    "1️⃣ `import scrapy` — Scrapy 라이브러리를 가져옵니다.  \n",
    "2️⃣ `class QuotesSpider(scrapy.Spider)` — Scrapy의 기본 `Spider` 클래스를 상속하여 사용자 정의 크롤러를 만듭니다.  \n",
    "3️⃣ `name = \"quotes\"` — 크롤러의 이름을 설정합니다. 터미널에서 이 이름으로 크롤러를 실행할 수 있습니다.  \n",
    "4️⃣ `start_urls = ['http://quotes.toscrape.com/tag/humor/']` — 크롤링을 시작할 URL 목록을 지정합니다.  \n",
    "5️⃣ `def parse(self, response)` — `parse` 메서드는 Scrapy가 페이지를 요청하고 응답을 처리하는 데 사용됩니다.  \n",
    "6️⃣ `for quote in response.css('div.quote')` — `CSS selector`를 사용하여 페이지의 `div.quote` 요소를 찾습니다.  \n",
    "7️⃣ `yield {'quote': quote.css('span.text::text').get()}` — 각 `div.quote` 요소에서 텍스트 데이터를 추출하고 결과를 딕셔너리 형태로 반환합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **실행 방법:**  \n",
    "1️⃣ 터미널에서 아래 명령어로 프로젝트를 생성합니다:  \n",
    "```bash\n",
    "scrapy startproject quotes_spider\n",
    "```\n",
    "\n",
    "2️⃣ **`spiders` 폴더**에 위 코드를 새 파일로 저장합니다 (예: `quotes_spider.py`).  \n",
    "\n",
    "3️⃣ 크롤러를 실행합니다:  \n",
    "```bash\n",
    "scrapy crawl quotes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "이 코드를 실행하면 **\"Humor\"** 태그가 있는 페이지에서 명언을 추출하여 출력합니다. 😊  \n",
    "Scrapy는 대규모 데이터 수집 프로젝트에 매우 유용한 라이브러리입니다! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db927d26",
   "metadata": {},
   "source": [
    "<a id=3></a>\n",
    "### **Selenium:**  \n",
    "Selenium은 웹 브라우저를 프로그래밍을 통해 제어하고 브라우저 작업을 자동화하는 도구입니다. 사용자는 웹 페이지에 직접 액세스하고 폼을 제출하거나 버튼을 클릭하는 등의 작업을 자동으로 수행할 수 있습니다. Selenium은 특히 **동적 콘텐츠**가 많은 웹사이트에서 유용합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **Selenium을 사용한 웹 브라우저 제어 예제 코드:**\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "# Firefox 브라우저 드라이버 실행\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# 특정 웹 페이지 열기\n",
    "driver.get(\"http://www.example.com\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **설명:**  \n",
    "1️⃣ **`from selenium import webdriver`** — Selenium의 `webdriver` 모듈을 가져옵니다.  \n",
    "\n",
    "2️⃣ **`driver = webdriver.Firefox()`** — Firefox 브라우저를 제어하기 위해 `Firefox` 드라이버를 실행합니다.  \n",
    "   - 다른 브라우저를 사용하려면 다음과 같이 변경할 수 있습니다:\n",
    "     - **Chrome:** `driver = webdriver.Chrome()`\n",
    "     - **Edge:** `driver = webdriver.Edge()`\n",
    "\n",
    "3️⃣ **`driver.get(\"http://www.example.com\")`** — 지정된 URL을 열고 브라우저 창에 웹 페이지를 표시합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **실행 방법:**  \n",
    "1️⃣ 먼저 Selenium 패키지를 설치합니다:  \n",
    "```bash\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "2️⃣ **웹 드라이버(WebDriver)**를 설치합니다.  \n",
    "   - Firefox용: **GeckoDriver**  \n",
    "   - Chrome용: **ChromeDriver**  \n",
    "\n",
    "**예:**  \n",
    "- Firefox 브라우저의 경우, **GeckoDriver**를 다운로드하고 시스템 PATH에 추가해야 합니다.  \n",
    "  다운로드 링크: [https://github.com/mozilla/geckodriver](https://github.com/mozilla/geckodriver)\n",
    "\n",
    "---\n",
    "\n",
    "### **Selenium을 활용한 기본 자동화 작업 예시:**  \n",
    "웹 페이지 열기, 입력 필드에 텍스트 입력, 버튼 클릭 등의 작업을 수행할 수 있습니다.\n",
    "\n",
    "#### 예제: Google 검색 자동화\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Chrome 브라우저 열기\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Google 홈페이지 열기\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "# 검색 입력 필드 찾기\n",
    "search_box = driver.find_element(\"name\", \"q\")\n",
    "\n",
    "# 검색어 입력 및 엔터키 누르기\n",
    "search_box.send_keys(\"Python 웹 스크래핑\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Selenium은 웹 사이트 자동화, 테스트 자동화, 데이터 수집에 매우 유용하며 **동적 웹 페이지**를 처리할 때 필수적인 도구입니다! 🧑‍💻✨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6a74b",
   "metadata": {},
   "source": [
    "### **웹 스크래핑의 활용 분야**  \n",
    "웹 스크래핑은 다양한 분야에서 활용되며 여러 응용 사례가 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 가격 비교 (Price Comparison)**  \n",
    "웹 스크래핑은 온라인 쇼핑 웹사이트에서 데이터를 수집하여 **제품 가격을 비교**하는 데 사용됩니다.  \n",
    "예를 들어, **ParseHub**와 같은 서비스는 여러 전자상거래 사이트의 가격 정보를 스크랩하여 사용자가 가장 저렴한 가격을 쉽게 찾을 수 있도록 도와줍니다.\n",
    "\n",
    "**사용 예시:**  \n",
    "- 여러 쇼핑몰의 상품 가격 비교 사이트  \n",
    "- 여행 및 항공권 가격 비교 플랫폼  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. 이메일 주소 수집 (Email Address Gathering)**  \n",
    "많은 기업이 **이메일 마케팅**을 위해 웹 스크래핑을 사용하여 웹사이트에서 이메일 주소를 수집합니다.  \n",
    "이렇게 수집한 이메일 주소를 활용하여 대량의 마케팅 이메일을 발송합니다.\n",
    "\n",
    "**사용 예시:**  \n",
    "- 뉴스레터 구독자 리스트 수집  \n",
    "- 고객 맞춤형 광고 이메일 발송  \n",
    "\n",
    "⚠️ **주의:** 이메일 주소를 무단으로 수집하고 사용하는 것은 **데이터 보호법**에 위배될 수 있습니다. 항상 법적 규제를 준수해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 소셜 미디어 스크래핑 (Social Media Scraping)**  \n",
    "소셜 미디어 사이트(예: **Twitter, Instagram, Facebook**)에서 데이터를 수집하여 **트렌드를 분석**하는 데 사용됩니다.  \n",
    "이를 통해 어떤 주제가 인기 있는지 파악하거나 **소비자 피드백**을 분석할 수 있습니다.\n",
    "\n",
    "**사용 예시:**  \n",
    "- 트위터 해시태그 및 키워드 분석  \n",
    "- 브랜드 이미지 관리 및 사용자 피드백 수집  \n",
    "- 인플루언서 및 트렌드 모니터링  \n",
    "\n",
    "---\n",
    "\n",
    "### **기타 활용 사례:**  \n",
    "✅ **뉴스 기사 수집** – 최신 뉴스를 자동으로 수집하여 분석  \n",
    "✅ **부동산 정보 수집** – 다양한 부동산 사이트에서 매물 정보를 수집하고 비교  \n",
    "✅ **리뷰 분석** – 제품 및 서비스에 대한 리뷰 데이터를 수집하여 소비자 만족도 분석  \n",
    "\n",
    "웹 스크래핑은 **데이터 기반 비즈니스 의사결정**에 큰 도움을 주며, 다양한 산업에서 점점 더 중요한 기술로 자리잡고 있습니다! 🚀😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364626dd",
   "metadata": {},
   "source": [
    "<a id=4></a>\n",
    "# Web Scraping Tables using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e36bca",
   "metadata": {},
   "source": [
    "### **Pandas의 `read_html()` 함수 소개**  \n",
    "**Pandas** 라이브러리의 `read_html()` 함수는 **웹 페이지**에서 **표 형식(tabular data)** 데이터를 추출하는 데 사용됩니다.  \n",
    "HTML 테이블 태그로 작성된 데이터를 자동으로 읽어와 Pandas의 **DataFrame** 형식으로 변환합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "### **예제: 세계 최대 은행 목록 추출하기**  \n",
    "아래는 Wikipedia 페이지에서 **시장 가치 기준 세계 최대 은행 목록**을 추출하는 코드입니다.\n",
    "\n",
    "#### 🔗 **데이터가 있는 URL:**  \n",
    "`https://en.wikipedia.org/wiki/List_of_largest_banks`\n",
    "\n",
    "---\n",
    "\n",
    "### **Python 코드:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# URL 설정\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "\n",
    "# read_html()로 모든 테이블 가져오기\n",
    "tables = pd.read_html(URL)\n",
    "\n",
    "# 첫 번째 테이블 출력\n",
    "largest_banks = tables[0]\n",
    "print(largest_banks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **설명:**  \n",
    "1️⃣ **`import pandas as pd`** — Pandas 라이브러리를 가져옵니다.  \n",
    "2️⃣ **`URL = 'https://en.wikipedia.org/wiki/List_of_largest_banks'`** — 데이터를 추출할 웹 페이지 URL을 설정합니다.  \n",
    "3️⃣ **`tables = pd.read_html(URL)`** — 지정된 URL에서 HTML 테이블을 모두 읽어와 **리스트 형태**로 저장합니다.  \n",
    "4️⃣ **`largest_banks = tables[0]`** — 첫 번째 테이블을 선택하여 **DataFrame**에 저장합니다.  \n",
    "5️⃣ **`print(largest_banks)`** — 추출된 테이블을 출력합니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed342a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Bank name</th>\n",
       "      <th>Total assets (2023) (US$ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Industrial and Commercial Bank of China</td>\n",
       "      <td>6303.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Agricultural Bank of China</td>\n",
       "      <td>5623.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>China Construction Bank</td>\n",
       "      <td>5400.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bank of China</td>\n",
       "      <td>4578.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>JPMorgan Chase</td>\n",
       "      <td>3875.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Raiffeisen Group</td>\n",
       "      <td>352.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Handelsbanken</td>\n",
       "      <td>351.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Industrial Bank of Korea</td>\n",
       "      <td>345.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>DNB</td>\n",
       "      <td>339.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Qatar National Bank</td>\n",
       "      <td>338.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                Bank name  \\\n",
       "0      1  Industrial and Commercial Bank of China   \n",
       "1      2               Agricultural Bank of China   \n",
       "2      3                  China Construction Bank   \n",
       "3      4                            Bank of China   \n",
       "4      5                           JPMorgan Chase   \n",
       "..   ...                                      ...   \n",
       "95    96                         Raiffeisen Group   \n",
       "96    97                            Handelsbanken   \n",
       "97    98                 Industrial Bank of Korea   \n",
       "98    99                                      DNB   \n",
       "99   100                      Qatar National Bank   \n",
       "\n",
       "    Total assets (2023) (US$ billion)  \n",
       "0                             6303.44  \n",
       "1                             5623.12  \n",
       "2                             5400.28  \n",
       "3                             4578.28  \n",
       "4                             3875.39  \n",
       "..                                ...  \n",
       "95                             352.87  \n",
       "96                             351.79  \n",
       "97                             345.81  \n",
       "98                             339.21  \n",
       "99                             338.14  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "tables = pd.read_html(URL)\n",
    "largest_banks = tables[0]\n",
    "largest_banks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce21cb",
   "metadata": {},
   "source": [
    "### **주의사항:**  \n",
    "✅ `read_html()` 함수는 **BeautifulSoup**을 내부적으로 사용하므로, **lxml** 또는 **html5lib**와 같은 파서가 필요합니다.  \n",
    "✅ 테이블이 많은 경우 원하는 테이블을 찾기 위해 **`tables[0]`, `tables[1]`**처럼 인덱싱을 사용해야 합니다.  \n",
    "✅ 일부 웹 페이지에서는 **JavaScript**로 동적 테이블을 생성하기 때문에 **Selenium**과 함께 사용해야 할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **장점:**  \n",
    "- HTML 테이블을 자동으로 **DataFrame**으로 변환하므로 추가 파싱 작업이 필요 없습니다.  \n",
    "- **데이터 분석 및 시각화** 작업으로 바로 이어질 수 있습니다.  \n",
    "\n",
    "---\n",
    "\n",
    "Pandas의 `read_html()` 함수는 **빠르고 간편하게 웹 데이터를 추출**할 수 있는 매우 강력한 도구입니다! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ff482",
   "metadata": {},
   "source": [
    "### **예시: 국가별 명목 GDP 목록 테이블 추출하기**  \n",
    "Pandas의 `read_html()` 함수는 **HTML 테이블을 DataFrame**으로 빠르게 변환하지만, 때로는 테이블 내에 있는 **하이퍼링크 텍스트**와 같은 불필요한 정보도 함께 추출됩니다. 이런 경우 **데이터 정제(cleaning)**가 필요합니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔗 **예제 URL:**  \n",
    "`https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Python 코드 예제:**  \n",
    "아래 코드는 위 URL에서 세 번째 테이블(index 2)을 추출하여 DataFrame에 저장하고 출력합니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# URL 설정\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
    "\n",
    "# 웹 페이지에서 모든 테이블 가져오기\n",
    "tables = pd.read_html(URL)\n",
    "\n",
    "# 인덱스가 2인 테이블 추출\n",
    "df = tables[2]\n",
    "\n",
    "# 데이터 출력\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **설명:**  \n",
    "1️⃣ **`pd.read_html()`** 함수는 웹 페이지에서 모든 테이블을 **리스트 형태**로 반환합니다.  \n",
    "2️⃣ **`tables[2]`** — 인덱스가 2인 테이블을 선택하여 DataFrame에 저장합니다.  \n",
    "3️⃣ **`print(df)`** — 추출된 데이터를 출력합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **출력 예시:**  \n",
    "|    | Country/Territory | GDP (US$ million) | Year |   |  \n",
    "|----|-------------------|-------------------|------|---|  \n",
    "|  0 | United States      | 25,350,000        | 2023 |   |  \n",
    "|  1 | China             | 18,321,000        | 2023 |   |  \n",
    "|  2 | Japan             | 4,940,000         | 2023 |   |  \n",
    "\n",
    "---\n",
    "\n",
    "### **하이퍼링크 및 기타 요소 처리:**  \n",
    "`read_html()` 함수로 추출한 데이터에는 **하이퍼링크 텍스트**나 기타 불필요한 요소가 포함될 수 있습니다.  \n",
    "이 경우 데이터를 정제(cleaning)하기 위해 Pandas의 **`str.replace()`**, **`apply()`** 등의 메서드를 사용하여 하이퍼링크나 기호를 제거할 수 있습니다.\n",
    "\n",
    "#### **하이퍼링크 텍스트 정리 예시:**\n",
    "```python\n",
    "# 'Country/Territory' 열에서 하이퍼링크 제거\n",
    "df['Country/Territory'] = df['Country/Territory'].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pandas와 BeautifulSoup의 차이점:**  \n",
    "| **Pandas (`read_html()`)**       | **BeautifulSoup**              |  \n",
    "|----------------------------------|--------------------------------|  \n",
    "| HTML **테이블** 데이터를 자동으로 추출 | HTML 페이지의 모든 요소 추출 가능 |  \n",
    "| 빠르고 간편함                     | 더 세부적인 데이터 추출 가능      |  \n",
    "| 정제 작업이 필요할 수 있음         | 더 많은 코드 작성이 필요함        |\n",
    "\n",
    "---\n",
    "\n",
    "### **핵심 포인트:**  \n",
    "- `read_html()`은 **HTML 테이블**을 손쉽게 DataFrame으로 가져오는 데 적합합니다.  \n",
    "- 웹 페이지의 **기타 요소(텍스트, 이미지, 링크)**를 추출하려면 **BeautifulSoup**이 더 적합합니다.  \n",
    "- 데이터 추출 후 Pandas의 **데이터 정제 기능**을 사용해 불필요한 데이터를 제거할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Pandas의 `read_html()`은 간단한 테이블 데이터 수집에 매우 유용하지만, 보다 복잡한 스크래핑 작업에는 **BeautifulSoup**을 사용하는 것이 더 적절합니다. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bcf8892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country/Territory</th>\n",
       "      <th colspan=\"2\" halign=\"left\">IMF[1][13]</th>\n",
       "      <th colspan=\"2\" halign=\"left\">World Bank[14]</th>\n",
       "      <th colspan=\"2\" halign=\"left\">United Nations[15]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country/Territory</th>\n",
       "      <th>Forecast</th>\n",
       "      <th>Year</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>1115494312</td>\n",
       "      <td>2025</td>\n",
       "      <td>105435540</td>\n",
       "      <td>2023</td>\n",
       "      <td>100834796</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>30337162</td>\n",
       "      <td>2025</td>\n",
       "      <td>27360935</td>\n",
       "      <td>2023</td>\n",
       "      <td>25744100</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China</td>\n",
       "      <td>19534894</td>\n",
       "      <td>[n 1]2025</td>\n",
       "      <td>17794782</td>\n",
       "      <td>[n 3]2023</td>\n",
       "      <td>17963170</td>\n",
       "      <td>[n 1]2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>4921563</td>\n",
       "      <td>2025</td>\n",
       "      <td>4456081</td>\n",
       "      <td>2023</td>\n",
       "      <td>4076923</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>4389326</td>\n",
       "      <td>2025</td>\n",
       "      <td>4212945</td>\n",
       "      <td>2023</td>\n",
       "      <td>4232173</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Kiribati</td>\n",
       "      <td>311</td>\n",
       "      <td>2024</td>\n",
       "      <td>279</td>\n",
       "      <td>2023</td>\n",
       "      <td>223</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Palau</td>\n",
       "      <td>308</td>\n",
       "      <td>2024</td>\n",
       "      <td>263</td>\n",
       "      <td>2023</td>\n",
       "      <td>225</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Marshall Islands</td>\n",
       "      <td>305</td>\n",
       "      <td>2024</td>\n",
       "      <td>284</td>\n",
       "      <td>2023</td>\n",
       "      <td>279</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Nauru</td>\n",
       "      <td>161</td>\n",
       "      <td>2024</td>\n",
       "      <td>154</td>\n",
       "      <td>2023</td>\n",
       "      <td>147</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>66</td>\n",
       "      <td>2024</td>\n",
       "      <td>62</td>\n",
       "      <td>2023</td>\n",
       "      <td>59</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country/Territory  IMF[1][13]            World Bank[14]             \\\n",
       "    Country/Territory    Forecast       Year       Estimate       Year   \n",
       "0               World  1115494312       2025      105435540       2023   \n",
       "1       United States    30337162       2025       27360935       2023   \n",
       "2               China    19534894  [n 1]2025       17794782  [n 3]2023   \n",
       "3             Germany     4921563       2025        4456081       2023   \n",
       "4               Japan     4389326       2025        4212945       2023   \n",
       "..                ...         ...        ...            ...        ...   \n",
       "205          Kiribati         311       2024            279       2023   \n",
       "206             Palau         308       2024            263       2023   \n",
       "207  Marshall Islands         305       2024            284       2023   \n",
       "208             Nauru         161       2024            154       2023   \n",
       "209            Tuvalu          66       2024             62       2023   \n",
       "\n",
       "    United Nations[15]             \n",
       "              Estimate       Year  \n",
       "0            100834796       2022  \n",
       "1             25744100       2022  \n",
       "2             17963170  [n 1]2022  \n",
       "3              4076923       2022  \n",
       "4              4232173       2022  \n",
       "..                 ...        ...  \n",
       "205                223       2022  \n",
       "206                225       2022  \n",
       "207                279       2022  \n",
       "208                147       2022  \n",
       "209                 59       2022  \n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL 설정\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
    "\n",
    "# 웹 페이지에서 모든 테이블 가져오기\n",
    "tables = pd.read_html(URL)\n",
    "\n",
    "# 인덱스가 2인 테이블 추출\n",
    "tables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b7260",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970204f",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "## Objectives\n",
    "* BeautifulSoup Python 라이브러리의 기본 개념을 숙지하고 있는 사람\n",
    "* 웹페이지에서 데이터를 스크래핑하고 필요한 데이터를 필터링할 수 있는 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e554181",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"BSO\">Beautiful Soup Object</a>\n",
    "            <ul>\n",
    "                <li>Tag</li>\n",
    "                <li>Children, Parents, and Siblings</li>\n",
    "                <li>HTML Attributes</li>\n",
    "                <li>Navigable String</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "     </ul>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"filter\">Filter</a>\n",
    "            <ul>\n",
    "                <li>find All</li>\n",
    "                <li>find </li>\n",
    "                <li>HTML Attributes</li>\n",
    "                <li>Navigable String</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "     </ul>\n",
    "     <ul>\n",
    "        <li>\n",
    "            <a href=\"DSCW\">Downloading And Scraping The Contents Of A Web</a>\n",
    "    </p>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb092acc",
   "metadata": {},
   "source": [
    "이번 실습에서는 Python과 여러 Python 라이브러리를 사용할 예정입니다. 이러한 라이브러리 중 일부는 실습 환경이나 SN Labs에 이미 설치되어 있을 수 있습니다. 그러나 다른 라이브러리는 직접 설치해야 할 수도 있습니다. 아래 셀을 실행하면 해당 라이브러리들이 설치됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b990f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4\n",
    "#!pip install requests pandas html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f5eb4",
   "metadata": {},
   "source": [
    "모듈을 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe68d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # web scrapping에 도움\n",
    "import requests  # 웹에서 페이지를 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b36c4",
   "metadata": {},
   "source": [
    "## Beautiful Soup Objects\n",
    "\n",
    "Beautiful Soup은 HTML 및 XML 파일에서 데이터를 추출하기 위한 Python 라이브러리입니다. 우리는 HTML 파일에 집중할 것입니다. 이 작업은 HTML을 객체 집합으로 표현하고, HTML을 파싱하기 위해 사용되는 메서드를 통해 이루어집니다. HTML을 트리 구조로 탐색하거나 원하는 데이터를 필터링할 수 있습니다.\n",
    "\n",
    "Consider the following HTML:\n",
    "```html\n",
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "<h3><b id='boldest'>Lebron James</b></h3>\n",
    "<p> Salary: $ 92,000,000 </p>\n",
    "<h3> Stephen Curry</h3>\n",
    "<p> Salary: $85,000, 000 </p>\n",
    "<h3> Kevin Durant </h3>\n",
    "<p> Salary: $73,200, 000</p>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd38cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700aa11",
   "metadata": {},
   "source": [
    "문서를 파싱하려면 BeautifulSoup 생성자에 문서를 전달하면 됩니다. BeautifulSoup 객체는 문서를 중첩된 데이터 구조로 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68e0d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78fe29",
   "metadata": {},
   "source": [
    "먼저, 문서는 유니코드(ASCII와 유사한 형식)로 변환되며 HTML 엔티티는 유니코드 문자로 변환됩니다. Beautiful Soup은 복잡한 HTML 문서를 Python 객체의 복잡한 트리 구조로 변환합니다. `BeautifulSoup` 객체는 다른 종류의 객체를 생성할 수 있습니다. 이번 실습에서는 `BeautifulSoup` 객체와 `Tag` 객체를 다룰 것이며, 이 두 객체는 이번 실습의 목적상 동일하게 취급됩니다. 마지막으로 `NavigableString` 객체에 대해서도 살펴보겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115167d7",
   "metadata": {},
   "source": [
    "`prettify()` 메서드를 사용하여 HTML을 중첩된 구조로 보기 좋게 출력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da193a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Page Title\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h3>\n",
      "   <b id=\"boldest\">\n",
      "    Lebron James\n",
      "   </b>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $ 92,000,000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Stephen Curry\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $85,000, 000\n",
      "  </p>\n",
      "  <h3>\n",
      "   Kevin Durant\n",
      "  </h3>\n",
      "  <p>\n",
      "   Salary: $73,200, 000\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f1782",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136e648",
   "metadata": {},
   "source": [
    "페이지의 제목과 최고 연봉 선수의 이름을 가져오고 싶다고 가정해봅시다. 이를 위해 `Tag` 객체를 사용할 수 있습니다. `Tag` 객체는 원본 문서의 HTML 태그에 해당하며, 예를 들어 `title` 태그가 이에 해당합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d1c6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object:<title>Page Title</title>\n"
     ]
    }
   ],
   "source": [
    "tag_object = soup.title\n",
    "print(f'tag object:{tag_object}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23cc8ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag object type: <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"tag object type: {type(tag_object)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c350dac",
   "metadata": {},
   "source": [
    "동일한 이름의 `Tag`가 여러 개 있는 경우, 해당 `Tag` 이름을 가진 첫 번째 요소가 호출됩니다. 이는 최고 연봉 선수에 해당합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12fd2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object = soup.h3\n",
    "tag_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c34d9",
   "metadata": {},
   "source": [
    "굵게 표시하는 속성인 `<b>` 태그 안에 포함되어 있으며, 트리 구조를 사용하면 더 쉽게 탐색할 수 있습니다. 자식(child) 속성을 사용하여 트리를 아래로 탐색하면 이름을 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8178d",
   "metadata": {},
   "source": [
    "### Children, Parents, and Siblings\n",
    "\n",
    "위에서 설명한 것처럼, `Tag` 객체는 객체들의 트리 구조로 되어 있습니다. 태그의 자식 요소에 접근하거나 브랜치를 따라 아래로 탐색할 때는 다음과 같이 할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00c2d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b id=\"boldest\">Lebron James</b>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child = tag_object.b\n",
    "tag_child"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabf73d",
   "metadata": {},
   "source": [
    "부모 요소에 접근하려면 `parent` 속성을 사용하면 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "366e499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_tag = tag_child.parent\n",
    "parent_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dd889",
   "metadata": {},
   "source": [
    "이는 다음과 동일합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a042cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3><b id=\"boldest\">Lebron James</b></h3>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919b0a2",
   "metadata": {},
   "source": [
    "`tag_object`의 부모 요소는 `body` 요소입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "808ba137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_object.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab4481",
   "metadata": {},
   "source": [
    "`tag_object`의 형제 요소는 `paragraph` 요소입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d64a352b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $ 92,000,000 </p>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_1 = tag_object.next_sibling\n",
    "\n",
    "sibling_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644da44c",
   "metadata": {},
   "source": [
    "`sibling_2`는 `header` 요소이며, 이는 `sibling_1`과 `tag_object`의 형제 요소이기도 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10704bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3> Stephen Curry</h3>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_2 = sibling_1.next_sibling\n",
    "sibling_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee9451",
   "metadata": {},
   "source": [
    "<h3 id=\"first_question\">Exercise: <code>next_sibling</code></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e625ef0",
   "metadata": {},
   "source": [
    "`Stephen Curry`의 연봉을 찾기 위해 객체 `sibling_2`와 메서드 `next_sibling`을 사용하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285c78cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p> Salary: $85,000, 000 </p>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sibling_2.next_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376727bd",
   "metadata": {},
   "source": [
    "### HTML Attributes\n",
    "\n",
    "\n",
    "태그에 속성이 있는 경우, `<code>id=\"boldest\"</code>` 태그에는 `id`라는 속성이 있으며, 그 값은 `boldest`입니다. 태그를 사전(dictionary)처럼 다루면 태그의 속성에 접근할 수 있습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e2b7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b id=\"boldest\">Lebron James</b>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d8fd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boldest'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6b1ae",
   "metadata": {},
   "source": [
    "해당 속성 사전에 직접 접근하려면 `attrs` 속성을 사용할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9daa6374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'boldest'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_child.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f91c60",
   "metadata": {},
   "source": [
    "### Navigable String\n",
    "\n",
    "문자열은 태그 안에 있는 텍스트 또는 콘텐츠에 해당합니다. Beautiful Soup은 이 텍스트를 담기 위해 `NavigableString` 클래스를 사용합니다. HTML에서 첫 번째 선수의 이름을 가져오려면 `Tag` 객체인 `tag_child`에서 문자열을 다음과 같이 추출할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38b1e15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_string = tag_child.string\n",
    "tag_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1161fbe",
   "metadata": {},
   "source": [
    "해당 타입이 `NavigableString`인지 확인할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76b98062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tag_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d66fe",
   "metadata": {},
   "source": [
    "`NavigableString`은 Python 문자열 또는 유니코드 문자열과 유사합니다. 보다 정확히 말하면, 주요 차이점은 `BeautifulSoup`의 몇 가지 기능도 지원한다는 점입니다. 이를 Python의 문자열 객체로 변환할 수 있습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7158b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lebron James'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_string = str(tag_string)\n",
    "unicode_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d13b9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unicode_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760b195",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "필터를 사용하면 복잡한 패턴을 찾을 수 있습니다. 가장 간단한 필터는 문자열입니다. 이 섹션에서는 문자열을 다양한 필터 메서드에 전달하고, Beautiful Soup이 해당 문자열과 정확히 일치하는 항목을 찾는 과정을 다룹니다. 예를 들어 다음과 같은 로켓 발사 HTML을 고려해 보세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d6a5ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <td id='flight' >Flight No</td>\n",
       "    <td>Launch site</td> \n",
       "    <td>Payload mass</td>\n",
       "   </tr>\n",
       "  <tr> \n",
       "    <td>1</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
       "    <td>300 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>2</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
       "    <td>94 kg</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>3</td>\n",
       "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n",
       "    <td>80 kg</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table>\n",
    "  <tr>\n",
    "    <td id='flight' >Flight No</td>\n",
    "    <td>Launch site</td> \n",
    "    <td>Payload mass</td>\n",
    "   </tr>\n",
    "  <tr> \n",
    "    <td>1</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
    "    <td>300 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
    "    <td>94 kg</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n",
    "    <td>80 kg</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e31c46",
   "metadata": {},
   "source": [
    "변수 `table`에 문자열로 위 html을 할당할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "822ee94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table=\"<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5d31080",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bs = BeautifulSoup(table, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f0bf48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><table><tbody><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr></tbody></table></body></html>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6f5ad",
   "metadata": {},
   "source": [
    "## find All\n",
    "\n",
    "`find_all()` 메서드는 태그의 하위 요소들을 검색하여 필터 조건과 일치하는 모든 하위 요소를 가져옵니다.\n",
    "\n",
    "\n",
    "`find_all()` 메서드의 시그니처는 다음과 같습니다:  \n",
    "`find_all(name, attrs, recursive, string, limit, **kwargs)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5561b09",
   "metadata": {},
   "source": [
    "### Name\n",
    "\n",
    "`name` 매개변수에 태그 이름을 설정하면, 해당 메서드는 지정한 이름의 모든 태그와 그 하위 요소들을 추출합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04191f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
